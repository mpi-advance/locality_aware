#!/bin/bash
#SBATCH --job-name gpu_alltoall_plus_copy_n2
#SBATCH --output gpu_alltoall_plus_copy_n2.%j.out
#SBATCH --error gpu_alltoall_plus_copy_n2.%j.err
#SBATCH -N 2
#SBATCH --gpus-per-node=4
#SBATCH --exclusive
#SBATCH --partition gpuA100x4
#SBATCH --time 00:40:00
#SBATCH --account=bebi-delta-gpu

module load nccl/2.19.3-1
module load cuda/12.4.0
module load openmpi/5.0.5+cuda

export CUDA_VISIBLE_DEVICES=3,2,1,0

cd $HOME/MPICCL/build/benchmarks

echo "1 Processes Per GPU, 4 GPUs Per Node"
mpirun --map-by ppr:1:numa --bind-to core --rank-by slot --display-map --display-allocation --report-bindings ./alltoall
echo "2 Processes Per GPU, 4 GPUs Per Node"
mpirun --map-by ppr:2:numa --bind-to core --rank-by slot --display-map --display-allocation --report-bindings ./alltoall_copy
echo "4 Processes Per GPU, 4 GPUs Per Node"
mpirun --map-by ppr:4:numa --bind-to core --rank-by slot --display-map --display-allocation --report-bindings ./alltoall_copy
echo "8 Processes Per GPU, 4 GPUs Per Node"
mpirun --map-by ppr:8:numa --bind-to core --rank-by slot --display-map --display-allocation --report-bindings ./alltoall_copy
echo "16 Processes Per GPU, 4 GPUs Per Node"
mpirun --map-by ppr:16:numa --bind-to core --rank-by slot --display-map --display-allocation --report-bindings ./alltoall_copy